{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8W8zcTKkUyi1",
    "outputId": "2b07c19f-f57f-4a84-e71c-7381021679b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n",
      "Go to Runtime > Change runtime and select a GPU hardware accelerator.\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"rnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1w6k1tuexBRf"
   },
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJqDiZvKwypM",
    "outputId": "b6906af9-a573-4806-bcfb-a91f768585df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/jsb_chorales.tgz\n",
      "122880/117661 [===============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "DOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/\"\n",
    "FILENAME = \"jsb_chorales.tgz\"\n",
    "filepath = keras.utils.get_file(FILENAME,\n",
    "                                DOWNLOAD_ROOT + FILENAME,\n",
    "                                cache_subdir=\"datasets/jsb_chorales\",\n",
    "                                extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wg_IxDD8xFu8"
   },
   "outputs": [],
   "source": [
    "jsb_chorales_dir = Path(filepath).parent\n",
    "train_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\n",
    "valid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\n",
    "test_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "em_CndBmxauM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_chorales(filepaths):\n",
    "    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n",
    "\n",
    "train_chorales = load_chorales(train_files)\n",
    "valid_chorales = load_chorales(valid_files)\n",
    "test_chorales = load_chorales(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "poTYdQ1ayB7X",
    "outputId": "92f8ac2b-0740-432c-ab63-4447a72c2310"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[74, 70, 65, 58],\n",
       " [74, 70, 65, 58],\n",
       " [74, 70, 65, 58],\n",
       " [74, 70, 65, 58],\n",
       " [75, 70, 58, 55],\n",
       " [75, 70, 58, 55],\n",
       " [75, 70, 60, 55],\n",
       " [75, 70, 60, 55],\n",
       " [77, 69, 62, 50],\n",
       " [77, 69, 62, 50],\n",
       " [77, 69, 62, 50],\n",
       " [77, 69, 62, 50],\n",
       " [77, 70, 62, 55],\n",
       " [77, 70, 62, 55],\n",
       " [77, 69, 62, 55],\n",
       " [77, 69, 62, 55],\n",
       " [75, 67, 63, 48],\n",
       " [75, 67, 63, 48],\n",
       " [75, 69, 63, 48],\n",
       " [75, 69, 63, 48],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [75, 69, 63, 48],\n",
       " [75, 69, 63, 48],\n",
       " [75, 67, 63, 48],\n",
       " [75, 67, 63, 48],\n",
       " [77, 65, 62, 50],\n",
       " [77, 65, 62, 50],\n",
       " [77, 65, 60, 50],\n",
       " [77, 65, 60, 50],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 53],\n",
       " [74, 67, 58, 53],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [74, 71, 53, 50],\n",
       " [74, 71, 53, 50],\n",
       " [74, 71, 53, 50],\n",
       " [74, 71, 53, 50],\n",
       " [75, 72, 55, 48],\n",
       " [75, 72, 55, 48],\n",
       " [75, 72, 55, 50],\n",
       " [75, 72, 55, 50],\n",
       " [75, 67, 60, 51],\n",
       " [75, 67, 60, 51],\n",
       " [75, 67, 60, 53],\n",
       " [75, 67, 60, 53],\n",
       " [74, 67, 60, 55],\n",
       " [74, 67, 60, 55],\n",
       " [74, 67, 57, 55],\n",
       " [74, 67, 57, 55],\n",
       " [74, 65, 59, 43],\n",
       " [74, 65, 59, 43],\n",
       " [72, 63, 59, 43],\n",
       " [72, 63, 59, 43],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [75, 67, 60, 60],\n",
       " [75, 67, 60, 60],\n",
       " [75, 67, 60, 60],\n",
       " [75, 67, 60, 60],\n",
       " [77, 70, 62, 58],\n",
       " [77, 70, 62, 58],\n",
       " [77, 70, 62, 56],\n",
       " [77, 70, 62, 56],\n",
       " [79, 70, 62, 55],\n",
       " [79, 70, 62, 55],\n",
       " [79, 70, 62, 53],\n",
       " [79, 70, 62, 53],\n",
       " [79, 70, 63, 51],\n",
       " [79, 70, 63, 51],\n",
       " [79, 70, 63, 51],\n",
       " [79, 70, 63, 51],\n",
       " [77, 70, 63, 58],\n",
       " [77, 70, 63, 58],\n",
       " [77, 70, 60, 58],\n",
       " [77, 70, 60, 58],\n",
       " [77, 70, 62, 46],\n",
       " [77, 70, 62, 46],\n",
       " [77, 68, 62, 46],\n",
       " [75, 68, 62, 46],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [75, 67, 58, 53],\n",
       " [75, 67, 58, 53],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [77, 65, 58, 50],\n",
       " [77, 65, 58, 50],\n",
       " [77, 65, 56, 50],\n",
       " [77, 65, 56, 50],\n",
       " [70, 63, 55, 51],\n",
       " [70, 63, 55, 51],\n",
       " [70, 63, 55, 51],\n",
       " [70, 63, 55, 51],\n",
       " [75, 65, 60, 45],\n",
       " [75, 65, 60, 45],\n",
       " [75, 65, 60, 45],\n",
       " [75, 65, 60, 45],\n",
       " [74, 65, 58, 46],\n",
       " [74, 65, 58, 46],\n",
       " [74, 65, 58, 46],\n",
       " [74, 65, 58, 46],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [75, 67, 58, 57],\n",
       " [75, 67, 58, 57],\n",
       " [75, 67, 58, 55],\n",
       " [75, 67, 58, 55],\n",
       " [77, 65, 60, 57],\n",
       " [77, 65, 60, 57],\n",
       " [77, 65, 60, 53],\n",
       " [77, 65, 60, 53],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46]]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chorales[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4hpXC3ayZxA"
   },
   "source": [
    "Notes range from 36 (C1 = C on octave 1) to 81 (A5 = A on octave 5), plus 0 for silence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "L9B9fHdYyGxC"
   },
   "outputs": [],
   "source": [
    "notes = set()\n",
    "for chorales in (train_chorales, valid_chorales, test_chorales):\n",
    "    for chorale in chorales:\n",
    "        for chord in chorale:\n",
    "            notes |= set(chord)\n",
    "\n",
    "n_notes = len(notes)\n",
    "min_note = min(notes - {0})\n",
    "max_note = max(notes)\n",
    "\n",
    "assert min_note == 36\n",
    "assert max_note == 81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aMK2we624a-"
   },
   "source": [
    "writing a few functions to listen to these chorales :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SQL68r3FydnR"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "def notes_to_frequencies(notes):\n",
    "    # Frequency doubles when you go up one octave; there are 12 semi-tones\n",
    "    # per octave; Note A on octave 4 is 440 Hz, and it is note number 69.\n",
    "    return 2 ** ((np.array(notes) - 69) / 12) * 440\n",
    "\n",
    "def frequencies_to_samples(frequencies, tempo, sample_rate):\n",
    "    note_duration = 60 / tempo # the tempo is measured in beats per minutes\n",
    "    # To reduce click sound at every beat, we round the frequencies to try to\n",
    "    # get the samples close to zero at the end of each note.\n",
    "    frequencies = np.round(note_duration * frequencies) / note_duration\n",
    "    n_samples = int(note_duration * sample_rate)\n",
    "    time = np.linspace(0, note_duration, n_samples)\n",
    "    sine_waves = np.sin(2 * np.pi * frequencies.reshape(-1, 1) * time)\n",
    "    # Removing all notes with frequencies ≤ 9 Hz (includes note 0 = silence)\n",
    "    sine_waves *= (frequencies > 9.).reshape(-1, 1)\n",
    "    return sine_waves.reshape(-1)\n",
    "\n",
    "def chords_to_samples(chords, tempo, sample_rate):\n",
    "    freqs = notes_to_frequencies(chords)\n",
    "    freqs = np.r_[freqs, freqs[-1:]] # make last note a bit longer\n",
    "    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n",
    "                     for melody in freqs.T], axis=0)\n",
    "    n_fade_out_samples = sample_rate * 60 // tempo # fade out last note\n",
    "    fade_out = np.linspace(1., 0., n_fade_out_samples)**2\n",
    "    merged[-n_fade_out_samples:] *= fade_out\n",
    "    return merged\n",
    "\n",
    "def play_chords(chords, tempo=160, amplitude=0.1, sample_rate=44100, filepath=None):\n",
    "    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n",
    "    if filepath:\n",
    "        from scipy.io import wavfile\n",
    "        samples = (2**15 * samples).astype(np.int16)\n",
    "        wavfile.write(filepath, sample_rate, samples)\n",
    "        return display(Audio(filepath))\n",
    "    else:\n",
    "        return display(Audio(samples, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghy-DcIF3yt2"
   },
   "source": [
    "Now let's listen to a few chorales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "Q2My9je93lK3",
    "outputId": "524b94c9-e132-49ca-81d5-962f18fe2f28"
   },
   "outputs": [],
   "source": [
    "for index in range(3):\n",
    "    play_chords(train_chorales[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AdYzWcte35jK"
   },
   "outputs": [],
   "source": [
    "def create_target(batch):\n",
    "    X = batch[:, :-1]\n",
    "    Y = batch[:, 1:] # predict next note in each arpegio, at each step\n",
    "    return X, Y\n",
    "\n",
    "def preprocess(window):\n",
    "    window = tf.where(window == 0, window, window - min_note + 1) # shift values\n",
    "    return tf.reshape(window, [-1]) # convert to arpegio\n",
    "\n",
    "def bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n",
    "                 window_size=32, window_shift=16, cache=True):\n",
    "    def batch_window(window):\n",
    "        return window.batch(window_size + 1)\n",
    "\n",
    "    def to_windows(chorale):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n",
    "        return dataset.flat_map(batch_window)\n",
    "\n",
    "    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n",
    "    dataset = dataset.flat_map(to_windows).map(preprocess)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(create_target)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoMWXPHhM4dO"
   },
   "source": [
    "creating the training set, the validation set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NYvIQiH6MePb"
   },
   "outputs": [],
   "source": [
    "train_set = bach_dataset(train_chorales, shuffle_buffer_size=1000)\n",
    "valid_set = bach_dataset(valid_chorales)\n",
    "test_set = bach_dataset(test_chorales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj7C2B7GPUqo"
   },
   "source": [
    "## building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vsueh4ehM-CW",
    "outputId": "6a597ea3-70c1-48f9-b9ea-e95b749cddf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 5)           235       \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 32)          352       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 48)          3120      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 48)          192       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 96)          12384     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 96)          384       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 256)         361472    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 47)          12079     \n",
      "=================================================================\n",
      "Total params: 396,810\n",
      "Trainable params: 396,330\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_embedding_dims = 5\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=n_notes, output_dim=n_embedding_dims,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.LSTM(256, return_sequences=True),\n",
    "    keras.layers.Dense(n_notes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ougFu4UBPXO8",
    "outputId": "bc57553c-36da-456c-b50d-18f2bf68c6cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "98/98 [==============================] - 71s 725ms/step - loss: 1.8144 - accuracy: 0.5367 - val_loss: 4.3396 - val_accuracy: 0.0418\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 64s 654ms/step - loss: 0.8932 - accuracy: 0.7638 - val_loss: 4.2530 - val_accuracy: 0.0919\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 70s 716ms/step - loss: 0.7502 - accuracy: 0.7926 - val_loss: 3.7055 - val_accuracy: 0.1556\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 67s 684ms/step - loss: 0.6760 - accuracy: 0.8080 - val_loss: 2.9389 - val_accuracy: 0.2039\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 70s 716ms/step - loss: 0.6251 - accuracy: 0.8192 - val_loss: 2.8066 - val_accuracy: 0.3081\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 71s 726ms/step - loss: 0.5850 - accuracy: 0.8275 - val_loss: 1.4126 - val_accuracy: 0.5617\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 72s 731ms/step - loss: 0.5476 - accuracy: 0.8369 - val_loss: 0.7185 - val_accuracy: 0.7951\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 70s 714ms/step - loss: 0.5153 - accuracy: 0.8449 - val_loss: 0.6295 - val_accuracy: 0.8179\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 67s 684ms/step - loss: 0.4855 - accuracy: 0.8522 - val_loss: 0.6660 - val_accuracy: 0.8069\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 71s 728ms/step - loss: 0.4601 - accuracy: 0.8590 - val_loss: 0.6127 - val_accuracy: 0.8228\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 67s 679ms/step - loss: 0.4340 - accuracy: 0.8665 - val_loss: 0.6112 - val_accuracy: 0.8230\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 61s 623ms/step - loss: 0.4109 - accuracy: 0.8732 - val_loss: 0.6406 - val_accuracy: 0.8130\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 74s 754ms/step - loss: 0.3897 - accuracy: 0.8791 - val_loss: 0.6930 - val_accuracy: 0.7991\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 72s 739ms/step - loss: 0.3723 - accuracy: 0.8841 - val_loss: 0.6085 - val_accuracy: 0.8226\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 70s 714ms/step - loss: 0.3511 - accuracy: 0.8905 - val_loss: 0.6181 - val_accuracy: 0.8217\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 70s 712ms/step - loss: 0.3334 - accuracy: 0.8960 - val_loss: 0.6255 - val_accuracy: 0.8182\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 63s 639ms/step - loss: 0.3160 - accuracy: 0.9016 - val_loss: 0.6634 - val_accuracy: 0.8117\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 75s 761ms/step - loss: 0.2999 - accuracy: 0.9064 - val_loss: 0.6358 - val_accuracy: 0.8179\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 71s 724ms/step - loss: 0.2900 - accuracy: 0.9090 - val_loss: 0.6334 - val_accuracy: 0.8216\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 69s 703ms/step - loss: 0.2693 - accuracy: 0.9160 - val_loss: 0.6759 - val_accuracy: 0.8157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5570bee7f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=20, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctIauHk7Pxuw",
    "outputId": "2b8a13ca-19f4-42c6-c186-1b127e2a6551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 5s 140ms/step - loss: 0.6875 - accuracy: 0.8089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6874654293060303, 0.8089456558227539]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"my_bach_model.h5\")\n",
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jFyDUQygQDAb"
   },
   "outputs": [],
   "source": [
    "def generate_chorale(model, seed_chords, length):\n",
    "    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n",
    "    arpegio = tf.reshape(arpegio, [1, -1])\n",
    "    for chord in range(length):\n",
    "        for note in range(4):\n",
    "            next_note = model.predict_classes(arpegio)[:1, -1:]\n",
    "            arpegio = tf.concat([arpegio, next_note], axis=1)\n",
    "    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
    "    return tf.reshape(arpegio, shape=[-1, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ1hPIaxREvf"
   },
   "source": [
    "test this function using the first 8 chords of one of the test chorales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "ESCKLsYgQqio",
    "outputId": "0995d176-4066-4498-8811-11b744ffdb07"
   },
   "outputs": [],
   "source": [
    "seed_chords = test_chorales[2][:8]\n",
    "play_chords(seed_chords, amplitude=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKB7y8LYRTUt"
   },
   "source": [
    "generate 56 more chords, for a total of 64 chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "id": "wzkljWHsRHoO",
    "outputId": "b655df1e-b5e5-421b-8c03-f4dafa92c91c"
   },
   "outputs": [],
   "source": [
    "new_chorale = generate_chorale(model, seed_chords, 56)\n",
    "play_chords(new_chorale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFNLCqdcXHb5"
   },
   "source": [
    "This approach has one major flaw: it is often too conservative. Indeed, the model will not take any risk, it will always choose the note with the highest score, and since repeating the previous note generally sounds good enough, it's the least risky option, so the algorithm will tend to make notes last longer and longer. Pretty boring. Plus, if you run the model multiple times, it will always generate the same melody.\n",
    "\n",
    "So let's spice things up a bit! Instead of always picking the note with the highest score, we will pick the next note randomly, according to the predicted probabilities. For example, if the model predicts a C3 with 75% probability, and a G3 with a 25% probability, then we will pick one of these two notes randomly, with these probabilities. We will also add a temperature parameter that will control how \"hot\" (i.e., daring) we want the system to feel. A high temperature will bring the predicted probabilities closer together, reducing the probability of the likely notes and increasing the probability of the unlikely ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TK4qJdM5RWmI"
   },
   "outputs": [],
   "source": [
    "def generate_chorale_v2(model, seed_chords, length, temperature=1):\n",
    "    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n",
    "    arpegio = tf.reshape(arpegio, [1, -1])\n",
    "    for chord in range(length):\n",
    "        for note in range(4):\n",
    "            next_note_probas = model.predict(arpegio)[0, -1:]\n",
    "            rescaled_logits = tf.math.log(next_note_probas) / temperature\n",
    "            next_note = tf.random.categorical(rescaled_logits, num_samples=1)\n",
    "            arpegio = tf.concat([arpegio, next_note], axis=1)\n",
    "    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
    "    return tf.reshape(arpegio, shape=[-1, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmiWO43VXe27"
   },
   "source": [
    "generating 3 chorales using this new function: one cold, one medium, and one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "M3P47zLfXXm3",
    "outputId": "20f0a567-5318-4124-dd59-1ba58f9b7135"
   },
   "outputs": [],
   "source": [
    "new_chorale_v2_cold = generate_chorale_v2(model, seed_chords, 56, temperature=0.8)\n",
    "play_chords(new_chorale_v2_cold, filepath=\"bach_cold.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "U68lpGKxXidx",
    "outputId": "a3e47904-6ac8-4f75-d4a1-7dcd91f63504"
   },
   "outputs": [],
   "source": [
    "new_chorale_v2_medium = generate_chorale_v2(model, seed_chords, 56, temperature=1.0)\n",
    "play_chords(new_chorale_v2_medium, filepath=\"bach_medium.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "FZHuI-WxXmV8",
    "outputId": "6e59faba-1a82-433a-9872-f90482b4200f"
   },
   "outputs": [],
   "source": [
    "new_chorale_v2_hot = generate_chorale_v2(model, seed_chords, 56, temperature=1.5)\n",
    "play_chords(new_chorale_v2_hot, filepath=\"bach_hot.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qe9B3aPeXwcf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Bach Chorales dataset ( generate new chorales using CNN , LSTM ) .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
